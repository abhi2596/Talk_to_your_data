{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents import create_csv_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import posixpath\n",
    "from operator import itemgetter\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    Union,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "from httpx import (\n",
    "    AsyncClient,\n",
    "    AsyncHTTPTransport,\n",
    "    Client,\n",
    "    HTTPTransport,\n",
    "    Limits,\n",
    "    Response,\n",
    ")\n",
    "from langchain_core._api import beta\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import (\n",
    "    BaseChatModel,\n",
    "    agenerate_from_stream,\n",
    "    generate_from_stream,\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessage,\n",
    "    ChatMessageChunk,\n",
    "    HumanMessage,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessage,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.output_parsers.base import OutputParserLike\n",
    "from langchain_core.output_parsers.openai_tools import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils import convert_to_secret_str\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "class ChatUnify(BaseChatModel):\n",
    "    \"\"\"ChatUnify chat model.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_unify import ChatUnify\n",
    "\n",
    "\n",
    "            model = ChatUnify(api_key=\"your-api-key\")\n",
    "            model.invoke(\"Hello, how are you?\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    client: Client = Field(default=None)\n",
    "    async_client: AsyncClient = Field(default=None)\n",
    "    unify_api_key: Optional[SecretStr] = None\n",
    "    unify_api_url: str = \"https://api.unify.ai/v0/\"\n",
    "    max_retries: int = 5\n",
    "    timeout: int = 120\n",
    "    max_concurrent_requests: int = 128\n",
    "\n",
    "    model: str = \"llama-2-70b-chat@lowest-input-cost\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of chat model.\"\"\"\n",
    "        return \"unify-chat\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "        }\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        unify_api_key = convert_to_secret_str(\n",
    "            values.get(\"unify_api_key\") or os.environ.get(\"UNIFY_API_KEY\") or \"\"\n",
    "        )\n",
    "        values[\"unify_api_key\"] = unify_api_key\n",
    "        values[\"client\"] = Client(\n",
    "            timeout=values.get(\"timeout\"),\n",
    "            transport=HTTPTransport(retries=values.get(\"max_retries\")),\n",
    "        )\n",
    "        values[\"async_client\"] = AsyncClient(\n",
    "            timeout=values.get(\"timeout\"),\n",
    "            limits=Limits(max_connections=values.get(\"max_concurrent_requests\")),\n",
    "            transport=AsyncHTTPTransport(retries=values.get(\"max_retries\")),\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    def _check_response(self, response: Response) -> None:\n",
    "        aread = False\n",
    "        if isinstance(self.client, AsyncClient):\n",
    "            aread = True\n",
    "        if response.status_code >= 500:\n",
    "            raise Exception(f\"Unify Server: Error {response.status}\")\n",
    "        elif response.status_code >= 400:\n",
    "            response.aread() if aread else response.read()\n",
    "            raise ValueError(f\"Unify received an invalid payload: {response.text}\")\n",
    "        elif response.status_code != 200:\n",
    "            response.aread() if aread else response.read()\n",
    "            raise Exception(\n",
    "                f\"Unify returned an unexpected response with status \"\n",
    "                f\"{response.status}: {response.text}\"\n",
    "            )\n",
    "\n",
    "    def _get_request_headers(self, stream) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"Accept\": \"text/event-stream\" if stream else \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.unify_api_key.get_secret_value()}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> List[Dict[str, Any]]:\n",
    "        formatted_messages = []\n",
    "        for message in messages:\n",
    "            formatted_message = {}\n",
    "            if isinstance(message, ChatMessage):\n",
    "                formatted_message[\"role\"] = message.role\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                formatted_message[\"role\"] = \"user\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                if \"tool_calls\" in message.additional_kwargs:\n",
    "                    formatted_message[\"tool_calls\"] = message.additional_kwargs[\n",
    "                        \"tool_calls\"\n",
    "                    ]\n",
    "                formatted_message[\"role\"] = \"assistant\"\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                formatted_message[\"role\"] = \"system\"\n",
    "            elif isinstance(message, ToolMessage):\n",
    "                formatted_message[\"role\"] = \"tool\"\n",
    "                formatted_message[\"name\"] = message.name\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported message type {message}\")\n",
    "            formatted_message[\"content\"] = message.content\n",
    "            formatted_messages.append(formatted_message)\n",
    "        return formatted_messages\n",
    "\n",
    "    def _convert_delta_to_message_chunk(\n",
    "        self, _delta: Dict, default_class: Type[BaseMessageChunk]\n",
    "    ) -> BaseMessageChunk:\n",
    "        role = _delta.get(\"role\")\n",
    "        content = _delta.get(\"content\", \"\")\n",
    "        if role == \"user\" or default_class == HumanMessageChunk:\n",
    "            return HumanMessageChunk(content=content)\n",
    "        elif role == \"assistant\" or default_class == AIMessageChunk:\n",
    "            additional_kwargs: Dict = {}\n",
    "            if _delta.get(\"tool_calls\"):\n",
    "                additional_kwargs[\"tool_calls\"] = _delta.get(\"tool_calls\")\n",
    "            return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)\n",
    "        elif role == \"system\" or default_class == SystemMessageChunk:\n",
    "            return SystemMessageChunk(content=content)\n",
    "        elif role or default_class == ChatMessageChunk:\n",
    "            return ChatMessageChunk(content=content, role=role)\n",
    "        else:\n",
    "            return default_class(content=content)\n",
    "\n",
    "    def parse_response_stream(self, default_chunk_class, line: str) -> ChatMessageChunk:\n",
    "        response_dict = line.removeprefix(\"data: \")\n",
    "        response_json = json.loads(response_dict)\n",
    "        choices = response_json[\"choices\"][0]\n",
    "        if not choices:\n",
    "            return None\n",
    "        delta = choices[\"delta\"]\n",
    "        if not delta.get(\"content\"):\n",
    "            return None\n",
    "        return self._convert_delta_to_message_chunk(delta, default_chunk_class)\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        headers = self._get_request_headers(True)\n",
    "        default_params = self._default_params\n",
    "        body = {\n",
    "            \"messages\": self._format_messages(messages),\n",
    "            \"stop\": stop,\n",
    "            \"stream\": True,\n",
    "            **default_params,\n",
    "            **kwargs,\n",
    "        }\n",
    "        print(body,headers)\n",
    "        url = posixpath.join(self.unify_api_url, \"chat/completions\")\n",
    "        with self.client.stream(\"post\", url, headers=headers, json=body) as response:\n",
    "            print(response)\n",
    "            self._check_response(response)\n",
    "            default_chunk_class = AIMessageChunk\n",
    "            for line in response.iter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                chunk = self.parse_response_stream(default_chunk_class, line)\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                default_chunk_class = chunk.__class__\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(token=chunk.content, chunk=chunk)\n",
    "                yield ChatGenerationChunk(message=chunk)\n",
    "\n",
    "    async def _acheck_response(self, response: Response) -> None:\n",
    "        if response.status_code >= 500:\n",
    "            raise Exception(f\"Unify Server: Error {response.status}\")\n",
    "        elif response.status_code >= 400:\n",
    "            raise ValueError(f\"Unify received an invalid payload: {response.text}\")\n",
    "        elif response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Unify returned an unexpected response with status \"\n",
    "                f\"{response.status}: {response.text}\"\n",
    "            )\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[ChatGenerationChunk]:\n",
    "        headers = self._get_request_headers(True)\n",
    "        default_params = self._default_params\n",
    "        body = {\n",
    "            \"messages\": self._format_messages(messages),\n",
    "            \"stop\": stop,\n",
    "            \"stream\": True,\n",
    "            **default_params,\n",
    "            **kwargs,\n",
    "        }\n",
    "        url = posixpath.join(self.unify_api_url, \"chat/completions\")\n",
    "        async with self.async_client.stream(\n",
    "            \"post\", url, headers=headers, json=body\n",
    "        ) as response:\n",
    "            await self._acheck_response(response)\n",
    "            default_chunk_class = AIMessageChunk\n",
    "            async for line in response.aiter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                chunk = self.parse_response_stream(default_chunk_class, line)\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                default_chunk_class = chunk.__class__\n",
    "\n",
    "                if run_manager:\n",
    "                    await run_manager.on_llm_new_token(token=chunk.content, chunk=chunk)\n",
    "                yield ChatGenerationChunk(message=chunk)\n",
    "\n",
    "    def _convert_to_message(self, _message: Dict[str, Any]) -> BaseMessage:\n",
    "        role = _message.get(\"role\")\n",
    "        content = cast(Union[str, List], _message.get(\"content\", \"\"))\n",
    "        if role == \"user\":\n",
    "            return HumanMessage(content=content)\n",
    "        elif role == \"assistant\":\n",
    "            additional_kwargs: Dict = {}\n",
    "            if _message.get(\"tool_calls\"):\n",
    "                additional_kwargs[\"tool_calls\"] = _message.get(\"tool_calls\")\n",
    "            return AIMessage(content=content, additional_kwargs=additional_kwargs)\n",
    "        elif role == \"system\":\n",
    "            return SystemMessage(content=content)\n",
    "        elif role == \"tool\":\n",
    "            return ToolMessage(content=content, name=_message.get(\"name\"))\n",
    "        else:\n",
    "            return ChatMessage(content=content, role=role)\n",
    "\n",
    "    def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n",
    "        generations = []\n",
    "        for res in data[\"choices\"]:\n",
    "            finish_reason = res.get(\"finish_reason\")\n",
    "            gen = ChatGeneration(\n",
    "                message=self._convert_to_message(res[\"message\"]),\n",
    "                generation_info={\"finish_reason\": finish_reason},\n",
    "            )\n",
    "            generations.append(gen)\n",
    "        usage = data.get(\"usage\")\n",
    "        return ChatResult(\n",
    "            generations=generations,\n",
    "            llm_output={\"usage\": usage, \"model\": self.model},\n",
    "        )\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        should_stream = stream if stream is not None else False\n",
    "        default_params = self._default_params\n",
    "        params = {**default_params, **kwargs}\n",
    "        if should_stream:\n",
    "            stream_iter = self._stream(\n",
    "                messages, stop=stop, run_manager=run_manager, **params\n",
    "            )\n",
    "            return generate_from_stream(stream_iter)\n",
    "        headers = self._get_request_headers(False)\n",
    "        body = {\"messages\": self._format_messages(messages), **params}\n",
    "        url = posixpath.join(self.unify_api_url, \"chat/completions\")\n",
    "        response = self.client.post(url, headers=headers, json=body)\n",
    "        self._check_response(response)\n",
    "        return self._format_output(response.json(), **kwargs)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        should_stream = stream if stream is not None else False\n",
    "        default_params = self._default_params\n",
    "        params = {**default_params, **kwargs}\n",
    "        if should_stream:\n",
    "            stream_iter = self._astream(\n",
    "                messages, stop=stop, run_manager=run_manager, **params\n",
    "            )\n",
    "            return await agenerate_from_stream(stream_iter)\n",
    "        headers = self._get_request_headers(False)\n",
    "        body = {\"messages\": self._format_messages(messages), **params}\n",
    "        url = posixpath.join(self.unify_api_url, \"chat/completions\")\n",
    "        response = await self.async_client.post(url, headers=headers, json=body)\n",
    "        self._check_response(response)\n",
    "        return self._format_output(response.json(), **kwargs)\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        return super().bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    @beta()\n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Union[Dict, Type[BaseModel]],\n",
    "        *,\n",
    "        include_raw: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n",
    "        if kwargs:\n",
    "            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n",
    "        is_pydantic_schema = isinstance(schema, type) and issubclass(schema, BaseModel)\n",
    "        llm = self.bind_tools([schema], tool_choice=\"any\")\n",
    "        if is_pydantic_schema:\n",
    "            output_parser: OutputParserLike = PydanticToolsParser(\n",
    "                tools=[schema], first_tool_only=True\n",
    "            )\n",
    "        else:\n",
    "            key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n",
    "            output_parser = JsonOutputKeyToolsParser(\n",
    "                key_name=key_name, first_tool_only=True\n",
    "            )\n",
    "\n",
    "        if include_raw:\n",
    "            parser_assign = RunnablePassthrough.assign(\n",
    "                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n",
    "            )\n",
    "            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n",
    "            parser_with_fallback = parser_assign.with_fallbacks(\n",
    "                [parser_none], exception_key=\"parsing_error\"\n",
    "            )\n",
    "            return RunnableMap(raw=llm) | parser_with_fallback\n",
    "        else:\n",
    "            return llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatUnify(unify_api_key=\"cUEAzLeS-sGwU83yIPMjkAK5YDJuDWpSRCLAqyQcqoM=\",model=\"gpt-3.5-turbo@openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-turbo@openai'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhij\\OneDrive\\Desktop\\UnifyAI_TD\\.venv\\lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:242: UserWarning: Received additional kwargs {'handle_parsing_errors': True} which are no longer supported.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True,handle_parsing_errors = True,agent_type=AgentType.OPENAI_FUNCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'messages': [{'role': 'system', 'content': '\\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\\nThis is the result of `print(df.head())`:\\n|    |   PassengerId |   Survived |   Pclass | Name                                                | Sex    |   Age |   SibSp |   Parch | Ticket           |    Fare | Cabin   | Embarked   |\\n|---:|--------------:|-----------:|---------:|:----------------------------------------------------|:-------|------:|--------:|--------:|:-----------------|--------:|:--------|:-----------|\\n|  0 |             1 |          0 |        3 | Braund, Mr. Owen Harris                             | male   |    22 |       1 |       0 | A/5 21171        |  7.25   | nan     | S          |\\n|  1 |             2 |          1 |        1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female |    38 |       1 |       0 | PC 17599         | 71.2833 | C85     | C          |\\n|  2 |             3 |          1 |        3 | Heikkinen, Miss. Laina                              | female |    26 |       0 |       0 | STON/O2. 3101282 |  7.925  | nan     | S          |\\n|  3 |             4 |          1 |        1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female |    35 |       1 |       0 | 113803           | 53.1    | C123    | S          |\\n|  4 |             5 |          0 |        3 | Allen, Mr. William Henry                            | male   |    35 |       0 |       0 | 373450           |  8.05   | nan     | S          |'}, {'role': 'user', 'content': 'how many rows are there?'}], 'stop': None, 'stream': True, 'model': 'gpt-4-turbo@openai', 'functions': [{'name': 'python_repl_ast', 'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. When using this tool, sometimes output is abbreviated - make sure it does not look abbreviated before using it in your answer.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'code snippet to run', 'type': 'string'}}, 'required': ['query']}}]} {'Accept': 'text/event-stream', 'Authorization': 'Bearer cUEAzLeS-sGwU83yIPMjkAK5YDJuDWpSRCLAqyQcqoM=', 'Content-Type': 'application/json'}\n",
      "<Response [200 OK]>\n",
      "\u001b[32;1m\u001b[1;3mTo determine the number of rows in the DataFrame `df`, you can use the `.shape` attribute or the `.len()` function. Since I can't execute code, I'll describe how you can find this information:\n",
      "\n",
      "1. **Using `.shape` attribute**: \n",
      "   - `df.shape` returns a tuple where the first element is the number of rows and the second is the number of columns. To get the number of rows, you would use `df.shape[0]`.\n",
      "\n",
      "2. **Using `len()` function**:\n",
      "   - `len(df)` directly returns the number of rows in the DataFrame.\n",
      "\n",
      "So, to find the number of rows in your DataFrame, you can execute either of the following lines of code in your Python environment:\n",
      "```python\n",
      "num_rows = df.shape[0]\n",
      "# or\n",
      "num_rows = len(df)\n",
      "\n",
      "print(num_rows)\n",
      "```\n",
      "\n",
      "Since I cannot execute code here and the provided data does not include total row count information, you would need to run this in your environment to get the exact number of rows.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how many rows are there?',\n",
       " 'output': \"To determine the number of rows in the DataFrame `df`, you can use the `.shape` attribute or the `.len()` function. Since I can't execute code, I'll describe how you can find this information:\\n\\n1. **Using `.shape` attribute**: \\n   - `df.shape` returns a tuple where the first element is the number of rows and the second is the number of columns. To get the number of rows, you would use `df.shape[0]`.\\n\\n2. **Using `len()` function**:\\n   - `len(df)` directly returns the number of rows in the DataFrame.\\n\\nSo, to find the number of rows in your DataFrame, you can execute either of the following lines of code in your Python environment:\\n```python\\nnum_rows = df.shape[0]\\n# or\\nnum_rows = len(df)\\n\\nprint(num_rows)\\n```\\n\\nSince I cannot execute code here and the provided data does not include total row count information, you would need to run this in your environment to get the exact number of rows.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
